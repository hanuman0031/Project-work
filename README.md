# RunAnywhere Web Starter App

A minimal React + TypeScript starter app demonstrating **on-device AI in the browser** using the [`@runanywhere/web`](https://www.npmjs.com/package/@runanywhere/web) SDK. All inference runs locally via WebAssembly â€” no server, no API key, 100% private.

## Features

| Tab | What it does |
|-----|-------------|
| **Chat** | Stream text from an on-device LLM (SmolLM2 360M) |
| **Vision** | Point your camera and describe what the VLM sees (LFM2-VL 450M) |
| **Voice** | Speak naturally â€” VAD detects speech, STT transcribes, LLM responds, TTS speaks back |
| **Code** | ðŸ†• AI Code Assistant with Tool Calling â€” write, execute, and debug code with live preview |

## Quick Start

```bash
npm install
npm run dev
```

Open [http://localhost:5173](http://localhost:5173). Models are downloaded on first use and cached in the browser's Origin Private File System (OPFS).

## How It Works

```
@runanywhere/web (npm package)
  â”œâ”€â”€ WASM engine (llama.cpp, whisper.cpp, sherpa-onnx)
  â”œâ”€â”€ Model management (download, OPFS cache, load/unload)
  â””â”€â”€ TypeScript API (TextGeneration, STT, TTS, VAD, VLM, VoicePipeline)
```

The app imports everything from `@runanywhere/web`:

```typescript
import { RunAnywhere, TextGeneration, VLMWorkerBridge } from '@runanywhere/web';

await RunAnywhere.initialize({ environment: 'development' });

// Stream LLM text
const { stream } = await TextGeneration.generateStream('Hello!', { maxTokens: 200 });
for await (const token of stream) { console.log(token); }

// VLM: describe an image
const result = await VLMWorkerBridge.shared.process(rgbPixels, width, height, 'Describe this.');
```

## Project Structure

```
src/
â”œâ”€â”€ main.tsx              # React root
â”œâ”€â”€ App.tsx               # Tab navigation (Chat | Vision | Voice)
â”œâ”€â”€ runanywhere.ts        # SDK init + model catalog + VLM worker
â”œâ”€â”€ workers/
â”‚   â””â”€â”€ vlm-worker.ts     # VLM Web Worker entry (2 lines)
â”œâ”€â”€ hooks/
â”‚   â””â”€â”€ useModelLoader.ts # Shared model download/load hook
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ChatTab.tsx        # LLM streaming chat
â”‚   â”œâ”€â”€ VisionTab.tsx      # Camera + VLM inference
â”‚   â”œâ”€â”€ VoiceTab.tsx       # Full voice pipeline
â”‚   â”œâ”€â”€ CodeTab.tsx        # ðŸ†• AI Code Assistant with Tool Calling
â”‚   â””â”€â”€ ModelBanner.tsx    # Download progress UI
â””â”€â”€ styles/
    â””â”€â”€ index.css          # Dark theme CSS
```

## Adding Your Own Models

Edit the `MODELS` array in `src/runanywhere.ts`:

```typescript
{
  id: 'my-custom-model',
  name: 'My Model',
  repo: 'username/repo-name',           // HuggingFace repo
  files: ['model.Q4_K_M.gguf'],         // Files to download
  framework: LLMFramework.LlamaCpp,
  modality: ModelCategory.Language,      // or Multimodal, SpeechRecognition, etc.
  memoryRequirement: 500_000_000,        // Bytes
}
```

Any GGUF model compatible with llama.cpp works for LLM/VLM. STT/TTS/VAD use sherpa-onnx models.

## ðŸ†• AI Code Assistant Feature

The **Code** tab demonstrates RunAnywhere's powerful **Tool Calling** capabilities:

- ðŸ’» **Live Code Editor** - Write HTML/CSS/JS with real-time preview
- ðŸ¤– **AI Assistant** - Chat with AI to generate, debug, and improve code
- ðŸ”§ **Tool Calling** - AI can execute JavaScript, update preview, and read current code
- ðŸ“š **Templates** - Quick start with pre-built examples (Counter, Animations, etc.)

See [CODE_ASSISTANT_README.md](./CODE_ASSISTANT_README.md) for full documentation.

### Example Interactions:
- "Create a todo list app" â†’ AI generates complete HTML/CSS/JS
- "Calculate fibonacci(10)" â†’ AI executes code and returns result
- "Add dark mode to this" â†’ AI reads current code and enhances it
- "Explain how flexbox works" â†’ AI creates interactive demo

## Deployment

### Vercel

```bash
npm run build
npx vercel --prod
```

The included `vercel.json` sets the required Cross-Origin-Isolation headers.

### Netlify

Add a `_headers` file:

```
/*
  Cross-Origin-Opener-Policy: same-origin
  Cross-Origin-Embedder-Policy: credentialless
```

### Any static host

Serve the `dist/` folder with these HTTP headers on all responses:

```
Cross-Origin-Opener-Policy: same-origin
Cross-Origin-Embedder-Policy: credentialless
```

## Browser Requirements

- Chrome 96+ or Edge 96+ (recommended: 120+)
- WebAssembly (required)
- SharedArrayBuffer (requires Cross-Origin Isolation headers)
- OPFS (for persistent model cache)

## Documentation

- [SDK API Reference](https://docs.runanywhere.ai)
- [npm package](https://www.npmjs.com/package/@runanywhere/web)
- [GitHub](https://github.com/RunanywhereAI/runanywhere-sdks)

## License

MIT
